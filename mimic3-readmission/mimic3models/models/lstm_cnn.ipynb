{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch import nn\n",
    "#import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-volume",
   "metadata": {},
   "source": [
    "# Some experiments with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    print(i, i.view(1, 1, -1))\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "print(inputs)\n",
    "print(out)\n",
    "print(hidden)\n",
    "print(hidden[0].shape)\n",
    "\n",
    "print(\"---------\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(inputs.shape)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-monster",
   "metadata": {},
   "source": [
    "# Define the LSTM+CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_size  = 50  # representing the one-hot encoded vector size\n",
    "#hidden_size = 100 # number of hidden nodes in the LSTM layer\n",
    "#n_layers    = 2   # number of LSTM layers\n",
    "#output_size = 50  # output of 50 scores for the next character\n",
    "\n",
    "#lstm   = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "#linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "# Data Flow Protocol\n",
    "# 1. network input shape: (batch_size, seq_length, num_features)\n",
    "# 2. LSTM output shape: (batch_size, seq_length, hidden_size)\n",
    "# 3. Linear input shape:  (batch_size * seq_length, hidden_size)\n",
    "# 4. Linear output: (batch_size * seq_length, out_size)\n",
    "\n",
    "#x = get_batches(data)         \n",
    "#x, hs = lstm(x, hs)\n",
    "#x = x.reshape(-1, hidden_size) \n",
    "#x = linear(x)\n",
    "\n",
    "class LSTM_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=390, hidden_dim=8, lstm_layers=1):\n",
    "\n",
    "        #dim, batch_norm, dropout, rec_dropout, task,\n",
    "        #target_repl = False, deep_supervision = False, num_classes = 1,\n",
    "        #depth = 1, input_dim = 390, ** kwargs\n",
    "\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = lstm_layers\n",
    "        self.bidirectional = True\n",
    "        #self.dense = dense\n",
    "\n",
    "        # some more parameters\n",
    "        #self.output_dim = dim\n",
    "        #self.batch_norm = batch_norm\n",
    "        self.dropout = 0.5\n",
    "        self.rec_dropout = 0.5\n",
    "        self.depth = lstm_layers\n",
    "        self.dropout_words = 0.3\n",
    "        self.dropout_rnn_U = 0.3\n",
    "        self.drop_conv = 0.5\n",
    "\n",
    "        # define the LSTM layer\n",
    "        # in keras we have inputs: A 3D tensor with shape [batch, timesteps, feature]\n",
    "        # units: Positive integer, dimensionality of the output space. = dim=num_units=hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.layers,\n",
    "                            dropout=self.rec_dropout,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                           batch_first=True)\n",
    "\n",
    "        # this is not in the original model\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        ##self.do1 = nn.Dropout(self.dropout)\n",
    "        ##self.cnn = nn.Conv1d()\n",
    "        # concat the three outputs from the CNN\n",
    "        ##self.do2 = nn.Dropout(self.drop_conv)\n",
    "        ##self.dense = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "\n",
    "        # not needed\n",
    "        # change linear layer inputs depending on if lstm is bidrectional\n",
    "        #if not bidirectional:\n",
    "        #    self.linear = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        #else:\n",
    "        #    self.linear = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        #self.act2 = nn.ReLU()\n",
    "\n",
    "        # change linear layer inputs depending on if lstm is bidrectional and extra dense layer isn't added\n",
    "        ##if bidirectional and not dense:\n",
    "        self.final = nn.Linear(self.hidden_dim * 2, 1)\n",
    "        ##else:\n",
    "        ##    self.final = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        out = inputs #.unsqueeze(1)\n",
    "        #print(\"inputs.shape = \", inputs.shape)\n",
    "        out, h = self.lstm(out)\n",
    "        #print(\"out lstm.shape = \", out.shape)\n",
    "        out = self.act1(out[:,-1])\n",
    "        #print(\"out relu.shape = \", out.shape)\n",
    "        #if self.dense:\n",
    "        #    out = self.linear(out)\n",
    "        #    out = self.act2(out)\n",
    "        out = self.final(out)\n",
    "        #print(\"out final.shape = \", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-tuning",
   "metadata": {},
   "source": [
    "# Load train and test data from Pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test data\n",
    "import pickle\n",
    "\n",
    "already_loaded = True\n",
    "try:\n",
    "    train_data\n",
    "except NameError as e:\n",
    "    already_loaded = False\n",
    "\n",
    "if not already_loaded:\n",
    "    train_data = pickle.load(open( \"../readmission/train_data/train_data\", \"rb\" ))\n",
    "    test_data = pickle.load(open( \"../readmission/train_data/test_data\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    print(train_data[1][idx],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions Train Data: \",len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]))\n",
    "print(\"Dimensions: \",len(test_data['data'][0]), len(test_data['data'][0][0]), len(test_data['data'][0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = test_data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-grenada",
   "metadata": {},
   "source": [
    "# Some analysis of the model and it's input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate some model\n",
    "model = LSTM_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = torch.Tensor(train_data[0][0])\n",
    "result = model(td.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-rings",
   "metadata": {},
   "source": [
    "# Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDataset(Dataset):\n",
    "    \"\"\"MIMIC dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data tuple(numpy.ndarray, list): data structured as tuple containing x which is a numpy array and y that is a list of values\n",
    "        \"\"\"\n",
    "        self.x = data[0]\n",
    "        self.y = data[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        x = torch.tensor(self.x[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "\n",
    "        return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0].shape, test_data['data'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-visit",
   "metadata": {},
   "source": [
    "# Instantiate both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = MIMICDataset(train_data)\n",
    "ds_test = MIMICDataset(test_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_train), len(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-wheel",
   "metadata": {},
   "source": [
    "# Create both dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(ds_train, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "dataloader_test = DataLoader(ds_test, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-sound",
   "metadata": {},
   "source": [
    "# Check if dataloaders work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in tqdm(enumerate(dataloader_train)):\n",
    "    print(type(sample_batched), len(sample_batched))\n",
    "    print(i_batch, sample_batched[0].shape)\n",
    "    inputs = sample_batched[0]\n",
    "    targets = sample_batched[1]\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs, targets)\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader_test):\n",
    "    print(type(sample_batched), len(sample_batched))\n",
    "    print(i_batch, sample_batched[0].shape)\n",
    "    inputs = sample_batched[0]\n",
    "    targets = sample_batched[1]\n",
    "    print(inputs.shape, targets.shape)\n",
    "    print(inputs, targets)\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-newman",
   "metadata": {},
   "source": [
    "# LSTM+CNN from Keras implementation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "printable-edition",
   "metadata": {},
   "source": [
    "<lstm_cnn.py.Network object at 0x7fcb15277a90>\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "X (InputLayer)                  (None, 48, 390)      0                                            \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_1 (Bidirectional) (None, 48, 16)       25536       X[0][0]                          \n",
    "__________________________________________________________________________________________________\n",
    "lstm_2 (LSTM)                   (None, 48, 16)       2112        bidirectional_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "dropout_1 (Dropout)             (None, 48, 16)       0           lstm_2[0][0]                     \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_1 (Conv1D)               (None, 47, 100)      3300        dropout_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_2 (Conv1D)               (None, 46, 100)      4900        dropout_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_3 (Conv1D)               (None, 45, 100)      6500        dropout_1[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1D)  (None, 23, 100)      0           conv1d_1[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1D)  (None, 23, 100)      0           conv1d_2[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_3 (MaxPooling1D)  (None, 22, 100)      0           conv1d_3[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "flatten_1 (Flatten)             (None, 2300)         0           max_pooling1d_1[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "flatten_2 (Flatten)             (None, 2300)         0           max_pooling1d_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "flatten_3 (Flatten)             (None, 2200)         0           max_pooling1d_3[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "merge_1 (Merge)                 (None, 6800)         0           flatten_1[0][0]                  \n",
    "                                                                 flatten_2[0][0]                  \n",
    "                                                                 flatten_3[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "dropout_2 (Dropout)             (None, 6800)         0           merge_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "dense_1 (Dense)                 (None, 1)            6801        dropout_2[0][0]                  \n",
    "==================================================================================================\n",
    "Total params: 49,149\n",
    "Trainable params: 49,149\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-sellers",
   "metadata": {},
   "source": [
    "# Train/Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop of the LSTM model\n",
    "\n",
    "def train(dataloader, model, optimizer, loss, device):\n",
    "    \"\"\"\n",
    "    main training function that trains model for one epoch/iteration cycle\n",
    "    Args:\n",
    "        :param dataloader: torch dataloader\n",
    "        :param model: model to train\n",
    "        :param optimizer: torch optimizer, e.g., adam, sgd, etc.\n",
    "        :param loss: torch loss, e.g., BCEWithLogitsLoss()\n",
    "        :param device: the target device, \"cuda\" oder \"cpu\"\n",
    "    \"\"\"\n",
    "    \n",
    "    total_loss = []\n",
    "    \n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # iterate over batches from dataloader\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Train epoch\"):\n",
    "        \n",
    "        # set inputs and targets\n",
    "        inputs = inputs.to(device, dtype=torch.float32)\n",
    "        targets = targets.to(device, dtype=torch.float32)\n",
    "        \n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass of inputs through the model\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss_ = loss(predictions, targets.view(-1,1))\n",
    "        \n",
    "        total_loss.append(loss_.item())\n",
    "        \n",
    "        # compute gradienta of loss w.r.t. to trainable parameters of the model\n",
    "        loss_.backward()\n",
    "        \n",
    "        # single optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss\n",
    "        \n",
    "def evaluate(dataloader, model, device):\n",
    "    \"\"\"\n",
    "    main eval function\n",
    "    Args:\n",
    "        :param dataloader: torch dataloader for test data set\n",
    "        :param model: model to evaluate\n",
    "        :param device: the target device, \"cuda\" oder \"cpu\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize empty lists to store predictions and targets\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    \n",
    "    # set model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Eval epoch\"):\n",
    "            # set inputs and targets\n",
    "            #inputs = inputs.unsqueeze(1)\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            targets = targets.to(device, dtype=torch.float32)\n",
    "            \n",
    "            # make predictions\n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            # move predicitions and targets to list\n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = targets.cpu().numpy().tolist()\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "            \n",
    "    # return final predicitions and targets\n",
    "    return final_predictions, final_targets\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-seven",
   "metadata": {},
   "source": [
    "# The training/evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_epochs = 20\n",
    "\n",
    "# create device depending which one is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fetch model\n",
    "model = LSTM_CNN()\n",
    "\n",
    "# send model to device\n",
    "model.to(device)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# initialize loss function\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Training Model\")\n",
    "\n",
    "best_accuracy = 0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(number_epochs):\n",
    "    # train one epoch\n",
    "    error = train(dataloader_train, model, optimizer, loss, device)\n",
    "    #validate\n",
    "    outputs, targets = evaluate(dataloader_test, model, device)\n",
    "    \n",
    "    #outputs = nn.ReLU()(torch.tensor(outputs)) #np.array(outputs) #>= 0.5\n",
    "    #accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    #print(f\"Epoch: {epoch}, Accuracy Score = {accuracy}, Loss = {loss.mean()}\")\n",
    "    o = nn.ReLU()(torch.tensor(outputs))\n",
    "    o = np.where(o.clone().detach().numpy() > 0.5, 1, 0)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(targets, o)\n",
    "    l = np.asarray(error)\n",
    "    print(f\"Epoch: {epoch}, Accuracy Score = {accuracy}, Loss = {l.mean()}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    #if early_stopping_counter > 2:\n",
    "    #    print(\"Early stopping done.\")\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-fireplace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
